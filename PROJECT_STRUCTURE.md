# Project Structure

## Directory Tree

```
nfl-streaming-pipeline/
â”‚
â”œâ”€â”€ ğŸ“ data/                          # Data fetching and storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ fetch_nfl_data.py            # Download NFL data from nflfastR
â”‚   â”œâ”€â”€ nfl_plays_2023.csv           # Full season data (generated)
â”‚   â”œâ”€â”€ nfl_games_2023.csv           # Game list (generated)
â”‚   â””â”€â”€ game_super_bowl_57.csv       # Sample game (generated)
â”‚
â”œâ”€â”€ ğŸ“ models/                        # ML training and model storage
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_models.py              # Train ML models
â”‚   â”œâ”€â”€ win_probability_model.pkl    # Logistic regression (generated)
â”‚   â”œâ”€â”€ score_model_home.pkl         # Linear regression home (generated)
â”‚   â”œâ”€â”€ score_model_away.pkl         # Linear regression away (generated)
â”‚   â”œâ”€â”€ feature_scaler.pkl           # StandardScaler (generated)
â”‚   â””â”€â”€ feature_columns.pkl          # Feature list (generated)
â”‚
â”œâ”€â”€ ğŸ“ producer/                      # Kafka producer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ kafka_producer.py            # Stream plays to Kafka
â”‚
â”œâ”€â”€ ğŸ“ consumer/                      # PySpark consumer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ spark_consumer.py            # Consume & predict
â”‚
â”œâ”€â”€ ğŸ“ config/                        # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py                  # Centralized config
â”‚
â”œâ”€â”€ ğŸ“ scripts/                       # Automation scripts
â”‚   â”œâ”€â”€ setup.sh                     # Initial setup
â”‚   â”œâ”€â”€ run_pipeline.sh              # Run full pipeline
â”‚   â””â”€â”€ cleanup.sh                   # Clean generated files
â”‚
â”œâ”€â”€ ğŸ“ predictions/                   # Prediction outputs (generated)
â”‚   â””â”€â”€ predictions_*.csv            # Timestamped predictions
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml            # Kafka infrastructure
â”œâ”€â”€ ğŸ“„ requirements.txt              # Python dependencies
â”œâ”€â”€ ğŸ“„ Makefile                      # Convenience commands
â”œâ”€â”€ ğŸ“„ .env                          # Environment variables
â”œâ”€â”€ ğŸ“„ .env.example                  # Environment template
â”œâ”€â”€ ğŸ“„ .gitignore                    # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“„ README.md                     # Main documentation
â”œâ”€â”€ ğŸ“„ QUICKSTART.md                 # 5-minute getting started
â”œâ”€â”€ ğŸ“„ ARCHITECTURE.md               # Technical deep dive
â”œâ”€â”€ ğŸ“„ PROJECT_SUMMARY.md            # Portfolio summary
â”œâ”€â”€ ğŸ“„ CONTRIBUTING.md               # Contribution guidelines
â””â”€â”€ ğŸ“„ LICENSE                       # MIT License
```

## File Purposes

### Core Application Files

| File | Lines | Purpose |
|------|-------|---------|
| `data/fetch_nfl_data.py` | ~250 | Download & clean NFL data |
| `models/train_models.py` | ~400 | Feature engineering & ML training |
| `producer/kafka_producer.py` | ~300 | Stream plays to Kafka |
| `consumer/spark_consumer.py` | ~400 | PySpark consumer with ML inference |

### Configuration Files

| File | Purpose |
|------|---------|
| `docker-compose.yml` | Kafka cluster setup (Zookeeper, Broker, UI) |
| `requirements.txt` | Python package dependencies |
| `.env` | Runtime configuration variables |
| `config/settings.py` | Centralized configuration loader |

### Documentation Files

| File | Purpose |
|------|---------|
| `README.md` | Complete project documentation (setup, usage, architecture) |
| `QUICKSTART.md` | Fast-track guide to get running in 5 minutes |
| `ARCHITECTURE.md` | Deep technical explanation of design decisions |
| `PROJECT_SUMMARY.md` | Portfolio presentation & talking points |
| `CONTRIBUTING.md` | Guidelines for contributors |

### Automation Scripts

| Script | Purpose |
|--------|---------|
| `scripts/setup.sh` | Automated initial setup (venv, Kafka, dependencies) |
| `scripts/run_pipeline.sh` | Run complete pipeline end-to-end |
| `scripts/cleanup.sh` | Stop services & clean generated files |
| `Makefile` | Convenience commands (make setup, make run, etc.) |

## Data Flow Through Files

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA FLOW                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. TRAINING PHASE (Offline)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   fetch_nfl_data.py
         â”‚
         â”œâ”€â†’ Downloads from nflfastR
         â”œâ”€â†’ Cleans & filters
         â””â”€â†’ Saves: data/nfl_plays_2023.csv
                â”‚
                â†“
   train_models.py
         â”‚
         â”œâ”€â†’ Loads CSV
         â”œâ”€â†’ Engineers features
         â”œâ”€â†’ Trains models
         â””â”€â†’ Saves: models/*.pkl
                â”‚
                â†“
   [Models ready for inference]

2. STREAMING PHASE (Online)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   kafka_producer.py
         â”‚
         â”œâ”€â†’ Reads: data/game_*.csv
         â”œâ”€â†’ Converts to JSON
         â””â”€â†’ Sends to Kafka topic
                â”‚
                â†“
   [Kafka broker stores messages]
                â”‚
                â†“
   spark_consumer.py
         â”‚
         â”œâ”€â†’ Consumes from Kafka
         â”œâ”€â†’ Loads: models/*.pkl
         â”œâ”€â†’ Engineers features
         â”œâ”€â†’ Predicts
         â”œâ”€â†’ Displays to console
         â””â”€â†’ Saves: predictions/*.csv
                â”‚
                â†“
   [Predictions available for analysis]
```

## Module Dependencies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEPENDENCIES                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

data/fetch_nfl_data.py
  â”œâ”€â”€ nfl_data_py (external)
  â””â”€â”€ pandas

models/train_models.py
  â”œâ”€â”€ pandas
  â”œâ”€â”€ numpy
  â”œâ”€â”€ sklearn
  â””â”€â”€ joblib

producer/kafka_producer.py
  â”œâ”€â”€ kafka-python
  â”œâ”€â”€ pandas
  â””â”€â”€ config/settings.py

consumer/spark_consumer.py
  â”œâ”€â”€ pyspark
  â”œâ”€â”€ pandas
  â”œâ”€â”€ joblib
  â””â”€â”€ config/settings.py

config/settings.py
  â”œâ”€â”€ python-dotenv
  â””â”€â”€ .env file
```

## Generated Files (Not in Git)

These files are created during setup and operation:

```
ğŸ“ data/
   â”œâ”€â”€ nfl_plays_2023.csv          (~50 MB)
   â”œâ”€â”€ nfl_games_2023.csv          (~50 KB)
   â””â”€â”€ game_super_bowl_57.csv      (~500 KB)

ğŸ“ models/
   â”œâ”€â”€ win_probability_model.pkl   (~2 MB)
   â”œâ”€â”€ score_model_home.pkl        (~2 MB)
   â”œâ”€â”€ score_model_away.pkl        (~2 MB)
   â”œâ”€â”€ feature_scaler.pkl          (~10 KB)
   â””â”€â”€ feature_columns.pkl         (~1 KB)

ğŸ“ predictions/
   â””â”€â”€ predictions_*_batch_*.csv   (~100 KB each)

ğŸ“ venv/                            (virtual environment)
   â””â”€â”€ ...                          (~500 MB)
```

## Docker Volumes

```
docker-compose.yml creates:

ğŸ“¦ kafka-data                       (Kafka logs & messages)
   â””â”€â”€ ...                          (~100 MB)
```

## Configuration Flow

```
1. .env.example
      â†“ (copy)
   .env
      â†“ (loaded by)
   config/settings.py
      â†“ (imported by)
   producer/kafka_producer.py
   consumer/spark_consumer.py
```

## Execution Order

### First-Time Setup
```
1. scripts/setup.sh
   â”œâ”€â†’ Creates venv
   â”œâ”€â†’ Installs requirements.txt
   â”œâ”€â†’ Copies .env.example â†’ .env
   â””â”€â†’ Starts docker-compose.yml

2. data/fetch_nfl_data.py
   â””â”€â†’ Downloads & processes data

3. models/train_models.py
   â””â”€â†’ Trains & saves models
```

### Running the Pipeline
```
Terminal 1:
  consumer/spark_consumer.py
     â†“
  [Waits for Kafka messages]

Terminal 2:
  producer/kafka_producer.py
     â†“
  [Streams plays to Kafka]
     â†“
  [Consumer receives & predicts]
```

## Size Breakdown

| Component | Size | Notes |
|-----------|------|-------|
| Source code | ~2 KB | Python files |
| Documentation | ~50 KB | Markdown files |
| Dependencies (venv) | ~500 MB | Python packages |
| Data files | ~50 MB | CSV files |
| Model files | ~10 MB | Serialized models |
| Docker volumes | ~100 MB | Kafka data |
| **Total** | **~650 MB** | Full project |

## Port Usage

| Port | Service | Purpose |
|------|---------|---------|
| 2181 | Zookeeper | Kafka coordination |
| 9092 | Kafka (internal) | Internal broker |
| 9093 | Kafka (external) | External connections |
| 8080 | Kafka UI | Web interface |

## Environment Variables

Referenced in `.env`:

| Variable | Default | Used By |
|----------|---------|---------|
| KAFKA_BOOTSTRAP_SERVERS | localhost:9093 | Producer, Consumer |
| KAFKA_TOPIC | nfl-plays | Producer, Consumer |
| NFL_SEASON | 2023 | Data fetcher |
| PLAY_DELAY_SECONDS | 0.5 | Producer |
| MAX_PLAYS_PER_GAME | 200 | Producer |
| SPARK_APP_NAME | NFL-ML-Predictions | Consumer |
| SPARK_LOG_LEVEL | WARN | Consumer |

## Quick Reference

### Start Services
```bash
make kafka-start        # Start Kafka
make fetch-data         # Download data
make train              # Train models
make consumer           # Run consumer (Terminal 1)
make producer           # Run producer (Terminal 2)
```

### Stop Services
```bash
Ctrl+C                  # Stop consumer/producer
make kafka-stop         # Stop Kafka
make clean              # Clean all generated files
```

### Check Status
```bash
make kafka-status       # Check Kafka containers
ls data/                # Check data files
ls models/              # Check model files
ls predictions/         # Check prediction outputs
```

## Code Statistics

```
Language: Python
Files: 8 (.py files)
Lines: ~1,500 (excluding comments)
Documentation: ~800 lines (markdown)
Comments: ~300 lines (inline)
```

## Testing Structure (Future)

```
tests/                              # To be implemented
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_feature_engineering.py
â”‚   â”œâ”€â”€ test_model_training.py
â”‚   â””â”€â”€ test_predictions.py
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â””â”€â”€ test_kafka_flow.py
â””â”€â”€ conftest.py                     # pytest fixtures
```

---

This structure is designed for:
- **Clarity:** Easy to navigate
- **Modularity:** Independent components
- **Scalability:** Add features without refactoring
- **Production-readiness:** Proper separation of concerns
- **Portfolio value:** Clear organization for reviewers
